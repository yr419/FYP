{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgwjLSw+Y/6+tp3PQSEixT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yr419/FYP/blob/main/toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7iOR7sgyFHQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib._color_data as mcd\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MDP parameters\n",
        "num_states = 4\n",
        "num_actions = 2\n",
        "discount_factor = 0.3\n",
        "\n",
        "# Define the transition probabilities and rewards for each action in each state\n",
        "# Here we use a toy example with deterministic transitions and rewards\n",
        "transitions = np.zeros((num_states, num_actions, num_states))\n",
        "rewards = np.zeros((num_states, num_actions, num_states))\n",
        "transitions[0, 0, 1] = 1.0\n",
        "rewards[0, 0, 1] = 5.0\n",
        "transitions[0, 1, 2] = 1.0\n",
        "rewards[0, 1, 2] = 10.0\n",
        "transitions[1, 0, 2] = 1.0\n",
        "rewards[1, 0, 2] = -1.0\n",
        "transitions[1, 1, 3] = 1.0\n",
        "rewards[1, 1, 3] = 2.0\n",
        "transitions[2, 0, 1] = 1.0\n",
        "rewards[2, 0, 1] = -3.0\n",
        "transitions[2, 1, 3] = 1.0\n",
        "rewards[2, 1, 3] = 7.0\n",
        "transitions[3, :, :] = 0.0  # terminal state\n",
        "\n",
        "# Define a function to simulate a single step in the MDP\n",
        "def step(state, action):\n",
        "    next_state = np.random.choice(num_states, p=transitions[state, action])\n",
        "    reward = rewards[state, action, next_state]\n",
        "    return next_state, reward\n",
        "\n",
        "# Define a function to evaluate a policy using value iteration\n",
        "def evaluate_policy(policy, theta=0.0001):\n",
        "    values = np.zeros(num_states)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in range(num_states):\n",
        "            v = values[state]\n",
        "            values[state] = sum(policy[state, action] * (rewards[state, action, next_state] + discount_factor * values[next_state])\n",
        "                                 for action in range(num_actions)\n",
        "                                 for next_state in range(num_states))\n",
        "            delta = max(delta, abs(v - values[state]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return values\n",
        "\n",
        "# Define a random policy to evaluate\n",
        "policy = np.ones((num_states, num_actions)) / num_actions\n",
        "\n",
        "# Evaluate the policy using value iteration\n",
        "values = evaluate_policy(policy)\n",
        "print(\"Values:\\n\", values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4ymSWEhz3gY",
        "outputId": "b85158df-c514-4302-ca83-eb57ea738dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values:\n",
            " [inf inf inf inf]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-7e8d2479ad24>:37: RuntimeWarning: overflow encountered in double_scalars\n",
            "  values[state] = sum(policy[state, action] * (rewards[state, action, next_state] + discount_factor * values[next_state])\n",
            "<ipython-input-18-7e8d2479ad24>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  delta = max(delta, abs(v - values[state]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_states = 4\n",
        "num_actions = 2\n",
        "\n",
        "# Initialize arrays for transitions and rewards\n",
        "transitions = np.zeros((num_states, num_actions, num_states))\n",
        "rewards = np.zeros((num_states, num_actions, num_states))\n",
        "\n",
        "# Define non-deterministic transitions and rewards for each state-action pair\n",
        "transitions[0, 0, [1, 2]] = [0.8, 0.2]  # action 0 in state 0 transitions to state 1 with probability 0.8 and state 2 with probability 0.2\n",
        "rewards[0, 0, [1, 2]] = [5.0, -2.0]    # corresponding rewards for the above transitions\n",
        "transitions[0, 1, [2, 3]] = [0.6, 0.4]  # action 1 in state 0 transitions to state 2 with probability 0.6 and state 3 with probability 0.4\n",
        "rewards[0, 1, [2, 3]] = [10.0, 1.0]    # corresponding rewards for the above transitions\n",
        "transitions[1, 0, [2, 3]] = [0.3, 0.7]  # ...\n",
        "rewards[1, 0, [2, 3]] = [-1.0, 3.0]\n",
        "transitions[1, 1, [0, 3]] = [0.9, 0.1]\n",
        "rewards[1, 1, [0, 3]] = [2.0, 4.0]\n",
        "transitions[2, 0, [0, 1]] = [0.4, 0.6]\n",
        "rewards[2, 0, [0, 1]] = [3.0, -2.0]\n",
        "transitions[2, 1, [1, 3]] = [0.5, 0.5]\n",
        "rewards[2, 1, [1, 3]] = [7.0, 2.0]\n",
        "transitions[3, :, :] = 0.0  # terminal state\n",
        "\n",
        "# Define the discount factor and the value function\n",
        "gamma = 0.9\n",
        "V = np.zeros(num_states)\n",
        "\n",
        "# Policy evaluation loop\n",
        "for i in range(1000):\n",
        "    # Update the value function for each state\n",
        "    for s in range(num_states):\n",
        "        V[s] = sum([transitions[s, a, s1] * (rewards[s, a, s1] + gamma * V[s1]) for a in range(num_actions) for s1 in range(num_states)])\n",
        "\n",
        "# Print the resulting value function\n",
        "print(V)"
      ],
      "metadata": {
        "id": "VUpZ1FBXWQll",
        "outputId": "cc6cf589-a9f2-4dd8-ce40-35dba35c5875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.12764673e+238 5.25497146e+238 7.04837457e+238 0.00000000e+000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define state space\n",
        "states = [0, 1, 2, 3, 4]\n",
        "\n",
        "# Define action space\n",
        "actions = [0, 1]\n",
        "\n",
        "step_n = 10 \n",
        "\n",
        "# Define transition function, (S,A)= (s': p(s'), s':p(s'))\n",
        "transition_function = {\n",
        "    (0, 0): {0: 0.4, 1: 0.6},\n",
        "    (0, 1): {1: 1.0},\n",
        "    (1, 0): {0: 0.3, 1: 0.7},\n",
        "    (1, 1): {2: 1.0},\n",
        "    (2, 0): {1: 0.5, 3: 0.5},\n",
        "    (2, 1): {3:0.9, 4: 0.1},\n",
        "    (3, 0): {0: 1.0},\n",
        "    (3, 1): {3:0.9, 4: 0.1},\n",
        "    (4, 0): {3: 1.0},\n",
        "    (4, 1): {4: 1.0},\n",
        "}\n",
        "\n",
        "# Define reward function\n",
        "reward_function = {\n",
        "    (0, 0, 0): -1,\n",
        "    (0, 1, 1): 1,\n",
        "    (1, 0, 0): -2,\n",
        "    (1, 1, 2): 2,\n",
        "    (2, 0, 1): -3,\n",
        "    (2, 1, 4): 10,\n",
        "    (3, 0, 0): 0,\n",
        "    (3, 1, 4): 5,\n",
        "    (4, 0, 3): -4,\n",
        "    (4, 1, 4): 0,\n",
        "}\n",
        "\n",
        "initial_state = 0\n",
        "\n",
        "# Define random policy\n",
        "def random_policy(state):\n",
        "    return random.choice(actions)\n",
        "    \n",
        "\n",
        "# Define step function with policy argument\n",
        "def step(state, policy):\n",
        "    action = policy(state)\n",
        "    next_states = transition_function[(state, action)]\n",
        "    next_state = random.choices(list(next_states.keys()), weights=list(next_states.values()))[0]\n",
        "    reward = reward_function.get((state, action, next_state), 0)\n",
        "    done = (next_state == 4)\n",
        "    return next_state, reward, done\n",
        "\n",
        "# Test the step function with random policy\n",
        "state = initial_state\n",
        "for i in range(10):\n",
        "    action = random_policy(state)\n",
        "    next_state, reward, done = step(state, random_policy)\n",
        "    print(f\"Step {i}: State {state} -> Action {action} -> Next state {next_state} -> Reward {reward}\")\n",
        "    if done:\n",
        "        print(\"Episode terminated\")\n",
        "        break\n",
        "    state = next_state\n",
        "\n",
        "# Define step function takes state and action, returns next state, reward, whether the episode is terminated\n",
        "#def step(state, action):\n",
        "#    next_states = transition_function[(state, action)]\n",
        " #   next_state = random.choices(list(next_states.keys()), weights=list(next_states.values()))[0]\n",
        " #   reward = reward_function.get((state, action, next_state), 0)\n",
        "#    done = (next_state == 4)\n",
        "#    return next_state, reward, done\n",
        "\n",
        "# Test the step function\n",
        "#state = initial_state\n",
        "#for i in range(step_n):\n",
        "#    action = random.choice(actions)\n",
        "#    next_state, reward, done = step(state, action)\n",
        "#    print(f\"Step {i}: State {state} -> Action {action} -> Next state {next_state} -> Reward {reward}\")\n",
        "#    if done:\n",
        "#        print(\"Episode terminated\")\n",
        "#        break\n",
        "#    state = next_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyKrAyJoaGrW",
        "outputId": "b474f730-ed78-4bf1-abcc-75569795c66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: State 0 -> Action 0 -> Next state 1 -> Reward 0\n",
            "Step 1: State 1 -> Action 1 -> Next state 2 -> Reward 2\n",
            "Step 2: State 2 -> Action 1 -> Next state 3 -> Reward 0\n",
            "Step 3: State 3 -> Action 1 -> Next state 3 -> Reward 0\n",
            "Step 4: State 3 -> Action 1 -> Next state 0 -> Reward 0\n",
            "Step 5: State 0 -> Action 1 -> Next state 1 -> Reward 1\n",
            "Step 6: State 1 -> Action 1 -> Next state 2 -> Reward 2\n",
            "Step 7: State 2 -> Action 0 -> Next state 1 -> Reward -3\n",
            "Step 8: State 1 -> Action 0 -> Next state 1 -> Reward 0\n",
            "Step 9: State 1 -> Action 0 -> Next state 1 -> Reward 0\n"
          ]
        }
      ]
    }
  ]
}